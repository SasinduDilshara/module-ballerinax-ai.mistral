// Copyright (c) 2025 WSO2 LLC (http://www.wso2.com).
//
// WSO2 LLC. licenses this file to you under the Apache License,
// Version 2.0 (the "License"); you may not use this file except
// in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

import ballerina/ai;
import ballerina/jballerina.java;
import ballerina/lang.regexp;
import ballerina/uuid;
import ballerinax/mistral;

const DEFAULT_MISTRAL_AI_SERVICE_URL = "https://api.mistral.ai/v1";
const DEFAULT_MAX_TOKEN_COUNT = 512;
const DEFAULT_TEMPERATURE = 0.7d;

# MistralAiProvider is a client class that provides an interface for interacting with Mistral AI Large Language Models.
public isolated client class Provider {
    *ai:ModelProvider;
    private final mistral:Client llmClient;
    private final string modelType;
    private final string apiKey;

    # # Initializes the Mistral AI model with the given connection configuration and model configuration.
    #
    # + apiKey - The Mistral AI API key
    # + modelType - The Mistral AI model name
    # + serviceUrl - The base URL of Mistral AI API endpoint
    # + maxTokens - The upper limit for the number of tokens in the response generated by the model
    # + temperature - The temperature for controlling randomness in the model's output
    # + connectionConfig - Additional HTTP connection configuration
    # + return - `nil` on successful initialization; otherwise, returns an `ai:Error`
    public isolated function init(@display {label: "API Key"} string apiKey,
            @display {label: "Model Type"} MISTRAL_AI_MODEL_NAMES modelType,
            @display {label: "Service URL"} string serviceUrl = DEFAULT_MISTRAL_AI_SERVICE_URL,
            @display {label: "Maximum Tokens"} int maxTokens = DEFAULT_MAX_TOKEN_COUNT,
            @display {label: "Temperature"} decimal temperature = DEFAULT_TEMPERATURE,
            @display {label: "Connection Configuration"} *ConnectionConfig connectionConfig
    ) returns ai:Error? {

        mistral:ConnectionConfig mistralConfig = {
            auth: {token: apiKey},
            httpVersion: connectionConfig.httpVersion,
            http1Settings: connectionConfig.http1Settings ?: {},
            http2Settings: connectionConfig?.http2Settings ?: {},
            timeout: connectionConfig.timeout,
            forwarded: connectionConfig.forwarded,
            poolConfig: connectionConfig?.poolConfig,
            cache: connectionConfig?.cache ?: {},
            compression: connectionConfig.compression,
            circuitBreaker: connectionConfig?.circuitBreaker,
            retryConfig: connectionConfig?.retryConfig,
            responseLimits: connectionConfig?.responseLimits ?: {},
            secureSocket: connectionConfig?.secureSocket,
            proxy: connectionConfig?.proxy,
            validation: connectionConfig.validation
        };

        mistral:Client|error llmClient = new (mistralConfig, serviceUrl);
        if llmClient is error {
            return error ai:Error("Failed to initialize MistralAiProvider", llmClient);
        }

        self.llmClient = llmClient;
        self.modelType = modelType;
        self.apiKey = apiKey;
    }

    # Uses function call API to determine next function to be called
    #
    # + messages - List of chat messages 
    # + tools - Tool definitions to be used for the tool call
    # + stop - Stop sequence to stop the completion
    # + return - Returns an array of ai:ChatAssistantMessage or an ai:LlmError in case of failures
    isolated remote function chat(ai:ChatMessage[] messages, ai:ChatCompletionFunctions[] tools, string? stop = ())
        returns ai:ChatAssistantMessage|ai:LlmError {
        MistralMessages[] mistralMessages = self.mapToMistralMessageRecords(messages);
        mistral:ChatCompletionRequest request = {model: self.modelType, stop, messages: mistralMessages};

        if tools.length() > 0 {
            mistral:Function[] mistralFunctions = [];
            foreach ai:ChatCompletionFunctions toolFunction in tools {
                mistral:Function mistralFunction = {
                    name: toolFunction.name,
                    description: toolFunction.description,
                    strict: false,
                    parameters: toolFunction.parameters ?: {}
                };
                mistralFunctions.push(mistralFunction);
            }

            mistral:Tool[] mistralTools = [];
            foreach mistral:Function mistralfunction in mistralFunctions {
                mistral:Tool mistralTool = {'function: mistralfunction};
                mistralTools.push(mistralTool);
            }
            request.tools = mistralTools;
        }

        mistral:ChatCompletionResponse|error response = self.llmClient->/chat/completions.post(request);
        if response is error {
            return error ai:LlmConnectionError("Error while connecting to the model", response);
        }
        return self.getAssistantMessage(response);
    }

    # Sends a chat request to the model and generates a value that belongs to the type
    # corresponding to the type descriptor argument.
    # 
    # + prompt - The prompt to use in the chat messages
    # + td - Type descriptor specifying the expected return type format
    # + return - Generates a value that belongs to the type, or an error if generation fails
    isolated remote function generate(ai:Prompt prompt, typedesc<anydata> td = <>) returns td|ai:Error = @java:Method {
        'class: "io.ballerina.lib.ai.mistral.Generator"
    } external;

    # Generates a random tool ID.
    #
    # + return - A random tool ID string
    private isolated function generateToolId() returns string {
        string randomToolId = "";
        string randomId = uuid:createRandomUuid();
        regexp:RegExp alphanumericPattern = re `[a-zA-Z0-9]`;
        int iterationCount = 0;

        foreach string character in randomId {
            if alphanumericPattern.isFullMatch(character) {
                randomToolId = randomToolId + character;
                iterationCount = iterationCount + 1;
            }
            if iterationCount == 9 {
                break;
            }
        }
        return randomToolId;
    }

    # Maps an array of `ai:ChatMessage` records to corresponding Mistral message records.
    #
    # + messages - Array of chat messages to be converted
    # + return - An `ai:LlmError` or an array of Mistral message records
    private isolated function mapToMistralMessageRecords(ai:ChatMessage[] messages) returns MistralMessages[] {
        MistralMessages[] mistralMessages = [];
        foreach ai:ChatMessage message in messages {
            if message is ai:ChatUserMessage {
                mistral:UserMessage userMessage = {role: ai:USER, content: message.content};
                mistralMessages.push(userMessage);
            } else if message is ai:ChatSystemMessage {
                mistral:SystemMessage systemMessage = {role: ai:SYSTEM, content: message.content};
                mistralMessages.push(systemMessage);
            } else if message is ai:ChatAssistantMessage {
                ai:FunctionCall[]? toolCalls = message.toolCalls;
                mistral:AssistantMessage mistralAssistantMessage = {role: ai:ASSISTANT, content: message.content};
                if toolCalls is ai:FunctionCall[] {
                    mistral:FunctionCall functionCall = {name: toolCalls[0].name, arguments: toolCalls[0].arguments.toJsonString()};
                    mistral:ToolCall[] toolCall = [{'function: functionCall, id: toolCalls[0]?.id ?: self.generateToolId()}];
                    mistralAssistantMessage.toolCalls = toolCall;
                }
                mistralMessages.push(mistralAssistantMessage);
            } else if message is ai:ChatFunctionMessage {
                mistral:ToolMessage mistralToolMessage = {
                    role: "tool",
                    content: message.content,
                    toolCallId: message.id ?: self.generateToolId()
                };
                mistralMessages.push(mistralToolMessage);
            }
        }
        return mistralMessages;
    }

    # Extracts assistant messages from a Mistral chat completion response.
    #
    # + response - The response from LLM
    # + return - An array of ai:ChatAssistantMessage records
    private isolated function getAssistantMessage(mistral:ChatCompletionResponse response)
        returns ai:ChatAssistantMessage|ai:LlmError {
        mistral:ChatCompletionChoice[]? choices = response.choices;
        if choices is () || choices.length() == 0 {
            return error ai:LlmInvalidResponseError("Empty response from the model when using function call API");
        }
        mistral:AssistantMessage message = choices[0].message;
        string|mistral:ContentChunk[]? content = message?.content;
        if content is mistral:TextChunk[]|mistral:DocumentURLChunk[]|mistral:ReferenceChunk[] {
            return error("Unsupported content type", cause = content);
        }
        string? stringContent = ();
        if content is string && content.length() > 0 {
            stringContent = content;
        } else if content is mistral:TextChunk[] {
            stringContent = string:'join("", ...content.'map(chunk => chunk.text));
        }

        mistral:ToolCall[]? toolCalls = message?.toolCalls;
        if toolCalls is () {
            return {role: ai:ASSISTANT, content: stringContent};
        }
        ai:FunctionCall[] functionCalls = [];
        foreach mistral:ToolCall toolcall in toolCalls {
            functionCalls.push(check self.mapToFunctionCall(toolcall));
        }
        return {role: ai:ASSISTANT, toolCalls: functionCalls, content: stringContent};
    }

    private isolated function mapToFunctionCall(mistral:ToolCall toolCall)
    returns ai:FunctionCall|ai:LlmError {
        do {
            record {}|string functionArgs = toolCall.'function.arguments;
            json jsonArgs = functionArgs is string ? check functionArgs.fromJsonString() : functionArgs.toJson();
            map<json>? arguments = check jsonArgs.cloneWithType();
            return {name: toolCall.'function.name, arguments, id: toolCall.id};

        } on fail error e {
            return error("Invalid or malformed arguments received in function call response.", e);
        }
    }
}
